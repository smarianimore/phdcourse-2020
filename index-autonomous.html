<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport">

    <title> Emerging IT Technologies: Autonomous Systems and Blockchains </title>
    <meta content="PhD course 'Emerging IT Technologies: Autonomous Systems and Blockchains' given by Stefano Mariani" name="description">
    <meta content="Stefano Mariani" name="author">

    <meta content="yes" name="apple-mobile-web-app-capable">
    <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">

    <link href="css/reset.css" rel="stylesheet">
    <link href="css/reveal.css" rel="stylesheet">
    <link href="css/theme/simple.css" rel="stylesheet">
    <!-- black, white, league, beige, sky, blood, night, serif, simple, solarized -->

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>

<body>
<div class="reveal">
    <div class="slides">
        <section data-autoslide="5000" data-transition="fade">

            <h2> Autonomous Systems: </h2>
            <br/>
            <h3> an overview </h3>
            <small><p><a href="index.html"> <em>(back to overview)</em> </a></p></small>
            <br/>
            <br/>
            <p><a href="https://smarianimore.github.io"> Stefano Mariani </a></p>
            <p><small><em> University of Modena and Reggio Emilia </em></small></p>

        </section>
        <section data-transition="convex" data-autoslide="2000">

            <h3> Outline </h3>

            <ol>
                <li class="fragment"> On the notion of autonomy </li>
                <li class="fragment"> Agents </li>
                <li class="fragment"> Multiagent systems </li>
                <li class="fragment"> Game theory: individual stance </li>
                <li class="fragment"> Game theory: collective stance </li>
                <li class="fragment"> Multiagent interactions </li>
                <li class="fragment"> Conclusion </li>
            </ol>

        </section>
        <section data-transition="convex">
            <section>

                <p class="fragment"> <span class="fragment highlight-red"> Why bother? </span> </p>
                <p class="fragment"> <strong>Social pressure</strong>: unproductive mind work and non-qualified time
                        consuming activities might be delegated to artificial systems </p>

            </section>
            <section>

                <h3> Issues </h3>

                <p class="fragment"> <span class="fragment highlight-red">Who does what?</span> </p>
                <p class="fragment"> Mostly, this is no longer an issue: <strong>artificial systems</strong> are
                    generally very welcome to do whatever we like </p>
                <p class="fragment"> <span class="fragment highlight-red">Who takes the decision?</span> </p>
                <p class="fragment"> Autonomy is at least about <strong>deliberation</strong> as
                    much as about action </p>
                <!--<li class="fragment"> <em>Executive vs. motivational autonomy</em> [Castelfranchi, 1995]: given a <strong>goal</strong>, an
                    "agent" is autonomous in achieving it by itself, vs. the agent's goals are somehow self-generated, not externally imposed </li>-->

            </section>
            <section>

                <img class="stretch" data-src="gartner-2019.png" style="text-align: left; float: left;">
                <br />
                <p style="text-align: right; float: right;">
                    <li class="fragment"> Autonomous driving </li>
                    <li class="fragment"> Decentralized Autonomous Organizations </li>
                    <li class="fragment"> Autonomous examination of data (advanced AI) </li>
                </p>

            </section>
            <section>

                <h3> Research questions </h3>

                <p class="fragment"> Is there just one single notion of <span class="fragment highlight-green">autonomy</span>? </p>
                <p class="fragment"> How do we <em>model</em> autonomy in artificial / computational systems? </p>
                <p class="fragment"> How do we <em>engineer</em> autonomy in artificial / computational systems? </p>

            </section>
            <section data-transition="zoom">

                <h2> On the notion of autonomy </h2>

            </section>
            <!--<section>

                <h3> Autonomy in context </h3>

                <p> According to [Maturana and Varela, 1980]: </p>

                <p> <em>In the domain of systems biology,
                    autonomous systems acquire the property of specifying <strong>their own rules</strong> of behaviour, do not work
                    as mere transducers or functions for converting input instructions into output products, and are
                    themselves sources of <strong>their own activity</strong></em> </p>

            </section>-->
            <section>

                <h3> Autonomy in context </h3>

                <p> From <a href="https://fas.org/irp/program/collect/usroadmap2011.pdf">[Unmanned Systems Integrated Roadmap FY 2011-2036]</a>: </p>
                <p> <em>Systems are self-directed toward a <span class="fragment highlight-green">goal</span> in that they do not require outside control, but rather are
                    governed by laws and strategies that direct their behavior</em> </p>
                <p class="fragment"> <em>These control algorithms are created and tested by teams of human operators and
                    software developers. However, if <strong>machine learning</strong>
                    is utilized, autonomous systems can develop modified strategies for themselves by which
                    they select their behavior</em> </p>

            </section>
            <section>

                <p> Various <strong>levels of autonomy</strong> in any system guide how much and how often
                    humans need to <em>interact or intervene</em> with the autonomous system:</p>
                <img class="stretch" data-src="unmanned-systems.png">

            </section>
            <section>

                <p> Level of <em>automation</em> for cars from <a href="https://www.sae.org/standards/content/j3016_201806/">[SAE J3016, 2018]</a>: </p>
                <img class="stretch" data-src="SAE-1.png">
                <small><p> ADS: automated driving system, <strong>DDT: dynamic driving task</strong>, ODD: operational design domain, <strong>OEDR: object and event detection and response</strong> </p></small>

            </section>
            <section>

                <img class="stretch" data-src="SAE-2.png">

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Agents </h2>

            </section>
            <!--<section>

                <h3> Software engineering perspective </h3>

                <ul>
                    <p> An <strong>object</strong> (e.g. in Java) is: </p>
                        <small><ul>
                            <li> state (instance or state variables) </li>
                            <li> methods (operations) </li>
                        </ul></small>
                    <p class="fragment"> Actually, other than state and methods: </p>
                        <small><ul class="fragment">
                            <li> internal threads </li>
                            <li> event handling </li>
                            <li> messaging </li>
                            <li> access to contextual information </li>
                        </ul></small>
                    <p class="fragment"> <span class="fragment highlight-red">Is it still an object?</span> </p>
                </ul>

            </section>
            <section>

                <ul>
                    <p> The "grown-up" objects and services of modern adaptive software are </p>
                        <small><ul class="fragment">
                            <li> not purely functional (they do not simply answer to request of services but rather try to achieve an
                                objective, a goal) </li>
                            <li> capable of unsolicited execution (due to internal threads) </li>
                            <li> <span class="fragment highlight-green">adaptive</span> (they can dynamically acquire information and tune their behavior accordingly) </li>
                            <li> <span class="fragment highlight-green">situated</span> (access to contextual and environmental information) </li>
                            <li> <span class="fragment highlight-green">social</span> (they interact with each other either via messaging or via mediated
                                interactions via the environment) </li>
                        </ul></small>
                    <p class="fragment"> This is very close to the definition of <span class="fragment highlight-green">agents</span>... </p>
                </ul>

            </section>-->
            <section>

                <h3> Software agents </h3>

                <p> Autonomy related to <strong>decision making</strong>: </p>
                <p class="fragment"> <strong>centralised</strong> decision making, as in service-oriented and object-based applications, achieves global goal via design by <em>delegation of
                    control</em> </p>
                <p class="fragment"> <strong>distributed</strong> decision making, as in agent-based applications, achieves global goal via
                    design by <em>delegation of responsibility</em> (<strong>agents can say no</strong>) </p>

            </section>
            <section>

                <p> A software agent is a component that is:

                    <small><ul>
                        <li class="fragment"> <strong>Goal-oriented</strong>: designed and deployed to achieve a specific goal (or to perform a specific task) </li>
                        <li class="fragment"> <strong>Autonomous</strong>: capable of acting in autonomy towards the achievement of its specific goals, without being
                            subject to a globally controlled thread of control </li>
                        <li class="fragment"> <strong>Situated</strong>: executes in the context of a specific environment
                            (computational or physical), and is able act in that environment by sensing and
                            affecting (via sensors and actuators) </li>
                        <li class="fragment"> <strong>Social</strong>: can interact with other agents in a multiagent system </li>
                    </ul></small>

            </section>
            <section>

                <h3> Autonomy in software agents </h3>


                <p> Agents can decide to autonomously activate towards the pursuing of the
                    goal, without the need of any specific event or solicitation: </p>
                <p class="fragment"> <strong>proactivity</strong> is a sort of extreme expression of autonomy </p>

            </section>
            <section>

                <p> Clearly, it is not always black and white, as modern objects have features
                    that can make objects resemble agents </p>
                <p> In effect, several systems for "agent-oriented programming" can be considered simply as advanced
                    tools for object-oriented programming or for more dynamic forms of service-oriented programming </p>

            </section>
            <section>

                <h3> Artificial intelligence perspective </h3>

                <p> For many researchers, agents do not simply have to be goal-oriented,
                    autonomous, situated, but they have to be <strong>"intelligent"</strong>, that typically means they have to
                    integrate "artificial intelligence" tools: neural networks, logic-based reasoning, conversational
                    capabilities (interact via natural language), ...  </p>

            </section>
            <section>

                <p> Treating a program as if it is intelligent (e.g. "the program <em>wants</em> the input
                    in a different format") is called the <strong>intentional stance</strong>, and it's helpful to us as programmers
                    to think this way </p>
                <p class="fragment"> The intentional stance leads us to program agents at the knowledge level (Newell),
                    that means reasoning about programs in terms of: </p>
                <small><ul class="fragment">
                    <li> facts and <span class="fragment highlight-green">beliefs</span> (rather than variables and data) </li>
                    <li> goals and <span class="fragment highlight-green">intentions</span> (rather than functionalities and methods) </li>
                    <li> <span class="fragment highlight-green">desires</span> and preferences (rather than external control) </li>
                </ul></small>

            </section>
            <section>

                <p> A possible definition of intelligence is: </p>
                <p> <em>"the capability to act purposefully towards the achievement of goals"</em> </p>
                <p class="fragment"> Hence for an agent to <em>be regarded (observed) as</em> being intelligent it is enough
                    simply to know how to achieve a given goal, which implies some sort of <strong>reasoning</strong> </p>

            </section>
            <section>

                <ul>
                    <p> E.g. a "smart" thermostat: </p>
                    <ul>
                        <li class="fragment"> Goal: keep temperature at 27° </li>
                        <li class="fragment"> Percept: temperature </li>
                        <li class="fragment"> Actions: increase / decrease temperature </li>
                        <li class="fragment"> Reasoning: <span><pre><code data-trim>
    if temp < 27:
        temp.increase()
    else if temp > 27:
        temp.decrease()
                            </code></pre></span></li>
                    </ul>
                    <p class="fragment"> Too simplistic, too low level of abstraction </p>
                </ul>

            </section>
            <section>

                <ul>
                    <p> Imagine a self-driving car: </p>
                    <ul class="fragment">
                        <li> Goal: don't kill pedestrians </li>
                        <li> Percept: current <em>situation</em> (where are pedestrians, other cars, weather conditions, ...) </li>
                        <li> Actions: gas, brake, steer, ... </li>
                        <li> <span class="fragment highlight-red">Reasoning</span>: still if-then-else?? </li>
                    </ul>
                    <p class="fragment"> Obviously not: <strong>theoretical</strong> + <strong>practical reasoning</strong> </p>
                </ul>

            </section>
            <section>

                <h3> Intelligence as "observing reasoning" </h3>

                <ul>
                    <li class="fragment"> <strong>Theoretical reasoning</strong>: a process which affects beliefs, to expand / improve
                        knowledge about what's happening (<em>inference</em>) </li>
                    <li class="fragment"> <strong>Practical reasoning</strong>: weighing conflicting considerations for and against
                        competing options, given what the agent desires and it believes (<em>decision making</em>) </li>
                    <small><ul>
                        <li class="fragment"> <strong>deliberation</strong>: decision about what goal to pursue (adopting <em>intentions</em>) </li>
                        <li class="fragment"> <strong>means-ends reasoning</strong>: decision about how to achieve goal (carry out actions) </li>
                    </ul></small>
                </ul>
                <p class="fragment"> Our focus will be on the latter </p>

            </section>
            <section>

                <h3> On rationality </h3>

                <p> Newell's Principle of <span class="fragment highlight-green">rationality</span>: </p>
                <p class="fragment"> if an agent has the knowledge that an action will
                    lead to the accomplishment of one of its goals (or to the maximization of its <span class="fragment highlight-green">utility</span>), then it will
                    select that action (<strong>Game Theory</strong> / <strong>Decision Theory</strong>) </p>

            </section>
            <section>

                <ul>
                    <p> <strong>BDI</strong> is a very successful and general model to "think" at software agents: </p>
                    <p class="fragment"> The agent has: </p>
                        <ul>
                            <li class="fragment"> <strong>Beliefs</strong> (what it knows about the world) </li>
                            <li class="fragment"> <strong>Desires</strong> (what it desires to happen) </li>
                            <li class="fragment"> <strong>Intentions</strong> (what it wants to do to satisfy its desires) </li>
                        </ul>
                </ul>

            </section>
            <section>

                <h3> Agent architectures </h3>

                <p> What types of architectures can we conceive for agents </p>
                <small><ul class="fragment">
                    <li> <strong>Reactive</strong> (or tropistic): "if-then-else" behaviour, simple and efficient, not very expressive, no real autonomy (agents act solely upon external stimuli) </li>
                    <li> Reactive with State (hysteretic): tracking the state of the world enables the agent to act in context, e.g. different actions with same stimulus </li>
                    <li> <strong>Goal-oriented</strong>: the agent plans its actions towards achievement of a desired state </li>
                    <li> <strong>Utility-oriented</strong>: the agent aims at maximising a function of some parameter measuring what it gains from a given state </li>
                </ul></small>
                </p>

            </section>
            <section>

                <img class="stretch" data-src="reactive.png">

            </section>
            <section>

                <img class="stretch" data-src="with-state.png">

            </section>
            <section>

                <img class="stretch" data-src="goal.png">

            </section>
            <section>

                <img class="stretch" data-src="utility.png">

            </section>
            <section>

                <h3> JADE example </h3>

                <p> <a href="https://www.researchgate.net/publication/226791220_Jade_-_A_Java_Agent_Development_Framework">JADE</a> stands for <strong>Java Agent DEvelopment Framework</strong> </p>
                <p> <a href="https://jade-project.gitlab.io/">http://jade.tilab.com/</a> </p>
                <p class="fragment"> JADE is a notable example of a
                    distributed, <strong>object-based</strong>, <strong>agent-oriented</strong> infrastructure,
                    hence an interesting example about how to face a design / programming <em>paradigm shift</em> </p>

            </section>
            <section>

                <ul>
                    <p> Despite being Java objects, JADE agents have a wide range of features
                        promoting their <strong>autonomy</strong>: </p>
                    <ul>
                        <li class="fragment"> each JADE agent is executed by a single Java thread </li>
                        <li class="fragment"> agents activities must be expressed in terms of <strong>behaviours</strong> </li>
                        <li class="fragment"> JADE agents communicate by exchanging messages </li>
                    </ul>
                </ul>

            </section>
            <section>

                <ul>
                    <p> A behaviour can be seen as an <em>activity to perform with the goal of
                        completing a task</em> </p>
                    <p class="fragment"> It can represent a <strong>proactive</strong> activity, started by the agent on
                        its own, as well as a <strong>reactive</strong> activity, performed in response to
                        some events (timeouts, messages, etc.) </p>
                    <p class="fragment"> Behaviours are Java objects executed
                        <strong>concurrently</strong> by a <strong>round-robin scheduler</strong> </p>
                </ul>

            </section>
            <section>

                <img class="stretch" data-src="jade-behaviours.png">

            </section>
            <section>

                <p><small> Get the code here: <a href="https://github.com/smarianimore/phdcourse-2020-jade">https://github.com/smarianimore/phdcourse-2020-jade</a> </small></p>

                <pre class="java"><code>
public class FSMAgent extends Agent {
	// State names
	private static final String STATE_A = "A";
	private static final String STATE_B = "B";
	private static final String STATE_C = "C";
	private static final String STATE_D = "D";
	private static final String STATE_E = "E";
	private static final String STATE_F = "F";

	protected void setup() {
		FSMBehaviour fsm = new FSMBehaviour(this) {
			public int onEnd() {
				System.out.println("FSM behaviour completed.");
				myAgent.doDelete();
				return super.onEnd();
			}
		};
		// Register state A (first state)
		fsm.registerFirstState(new NamePrinter(), STATE_A);
		// Register state B
		fsm.registerState(new NamePrinter(), STATE_B);
		// Register state C
		fsm.registerState(new RandomGenerator(3), STATE_C);
		// Register state D
		fsm.registerState(new NamePrinter(), STATE_D);
		// Register state E
		fsm.registerState(new RandomGenerator(4), STATE_E);
		// Register state F (final state)
		fsm.registerLastState(new NamePrinter(), STATE_F);

		// Register the transitions
		fsm.registerDefaultTransition(STATE_A, STATE_B);
		fsm.registerDefaultTransition(STATE_B, STATE_C);
		fsm.registerTransition(STATE_C, STATE_C, 0);
		fsm.registerTransition(STATE_C, STATE_D, 1);
		fsm.registerTransition(STATE_C, STATE_A, 2);
		fsm.registerDefaultTransition(STATE_D, STATE_E);
		fsm.registerTransition(STATE_E, STATE_F, 3);
		fsm.registerDefaultTransition(STATE_E, STATE_B);

		addBehaviour(fsm);
	}

	/**
	   Inner class NamePrinter.
	   This behaviour just prints its name
	 */
	private class NamePrinter extends OneShotBehaviour {
		public void action() {
			System.out.println("Executing behaviour "+getBehaviourName());
		}
	}

	/**
	   Inner class RandomGenerator.
	   This behaviour prints its name and exits with a random value
	   between 0 and a given integer value
	 */
	private class RandomGenerator extends NamePrinter {
		private int maxExitValue;
		private int exitValue;

		private RandomGenerator(int max) {
			super();
			maxExitValue = max;
		}

		public void action() {
			System.out.println("Executing behaviour "+getBehaviourName());
			exitValue = (int) (Math.random() * maxExitValue);
			System.out.println("Exit value is "+exitValue);
		}

		public int onEnd() {
			return exitValue;
		}
	}
}
                </code></pre>

            </section>
            <section>

                <h3> Jason example </h3>

                <p> Jason is a <strong>BDI agent programming</strong> language and development framework </p>
                <p> <a href="http://jason.sourceforge.net">http://jason.sourceforge.net</a> </p>
                <ul>
                    <p class="fragment"> Jason language is a variant of <a href="https://link.springer.com/chapter/10.1007/11750734_9">AgentSpeak</a>, whose main constructs are: </p>
                    <ul>
                        <li class="fragment"> <strong>beliefs</strong> </li>
                        <li class="fragment"> <strong>goals</strong> (the desires of BDI) </li>
                        <li class="fragment"> <strong>plans</strong>, as lists of actions achieving a given goal (the intentions of BDI) </li>
                    </ul>
                </ul>


            </section>
            <section>

                <img class="stretch" data-src="jason-arch.png">

            </section>
            <section>

                <ul>
                    <p> Jason agent reasoning cycle: </p>
                    <small><ol>
                        <li> perceiving the environment </li>
                        <li> updating the belief base </li>
                        <li> receiving communication from other agents </li>
                        <li> selecting ‘socially acceptable’ messages </li>
                        <li> selecting an event </li>
                        <li> retrieving all relevant plans </li>
                        <li> determining the applicable plans </li>
                        <li> selecting one applicable plan </li>
                        <li> selecting an intention for further execution </li>
                        <li> executing one step of an intention </li>
                    </ol></small>
                </ul>

            </section>
            <section>

                <p><small> Get the code here: <a href="https://gitlab.com/pika-lab/courses/as/ay1920/jason-agents">https://gitlab.com/pika-lab/courses/as/ay1920/jason-agents</a> </small></p>

                <pre class="prolog"><code>
target(20).

+temperature(X) <- !regulate_temperature(X).

+!regulate_temperature(X) : target(Y) & X - Y > 0.5 <-
    .print("Temperature is ", X, ": need to cool down");
    spray_air(cold).

+!regulate_temperature(X) : target(Y) & Y - X > 0.5 <-
    .print("Temperature is ", X, ": need to warm up");
    spray_air(hot).

+!regulate_temperature(X) : target(Y) & Z = X - Y & Z >= -0.5 & Z <= 0.5 <-
    .print("Temperature is ", X, ": it's ok.").

-!regulate_temperature(X) <-
    .print("Failed to spray air. Retrying.");
    !regulate_temperature(X).
                </code></pre>

                <p><small>(kindly provided by <a href="https://about.me/gciatto">Giovanni Ciatto</a>)</small></p>

            </section>
            <section>

                <p><small> Get the code here: <a href="https://gitlab.com/pika-lab/courses/as/ay1920/jason-agents">https://gitlab.com/pika-lab/courses/as/ay1920/jason-agents</a> </small></p>

                <pre class="java"><code><script type="text/template">
public class TemperatureEnvironment extends Environment {

    private static final Random RAND = new Random();

    // action literals
    public static final Literal hotAir = Literal.parseLiteral("spray_air(hot)");
    public static final Literal coldAir = Literal.parseLiteral("spray_air(cold)");

    static Logger logger = Logger.getLogger(TemperatureEnvironment.class.getName());

    private double temperature;

    @Override
    public void init(final String[] args) {
        if (args.length >= 1) {
            temperature = Double.parseDouble(args[0]);
        } else {
            temperature = RAND.nextDouble() * 20 + 10;
        }
    }

    @Override
    public Collection<       > getPercepts(String agName) {
        return Collections.singletonList(
                Literal.parseLiteral(String.format("temperature(%s)", temperature))
        );
    }

    private static final double FAILURE_PROBABILITY = 0.2;

    @Override
    public boolean executeAction(final String ag, final Structure action) {
        boolean result = true;
        if (RAND.nextDouble() < FAILURE_PROBABILITY) {
            result = false;
        } else if (action.equals(hotAir)) {
            temperature += 0.1;
        } else if (action.equals(coldAir)) {
            temperature -= 0.1;
        } else {
            RuntimeException e = new IllegalArgumentException("Cannot handle action: " + action);
            logger.warning(e.getMessage());
            throw e;
        }
        try {
            Thread.sleep(500L); // Slowdown the system
        } catch (InterruptedException ignored) {
        }
        return result;
    }
}
                </script></code></pre>

                <p><small>(kindly provided by <a href="https://about.me/gciatto">Giovanni Ciatto</a>)</small></p>

            </section>
            <section>

                <h3> Autonomy <=> adaptiveness </h3>

                <p> What is <span class="fragment highlight-red">adaptiveness</span>? </p>
                <p class="fragment"> The capability of a system and/or of its individuals of <strong>changing its behavior</strong> according to contingent situations </p>
                <small><ul>
                    <li class="fragment"> re-organizes itself in response to stimuli (<strong>flexibility</strong>) </li>
                    <li class="fragment"> preserves the same overall dynamics (<strong>robustness</strong>) </li>
                </ul></small>
                <p class="fragment"><em>Autonomy (of decision making) enables adaptiveness, and adaptiveness is a way of exhibiting autonomy, in turn!</em> </p>

            </section>
            <section>

                <p> Can be achieved via <strong>context-awareness</strong>: knowledge of the environment, conditions under which
                the agent operates </p>
                <p class="fragment"> <em>As <strong>situated</strong> components, agents are inherently context-aware!</em> </p>
                <p class="fragment"> Where agents cannot reach, suitable middleware provides desired services... </p>

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Multiagent systems </h2>

            </section>
            <section>

                <h3> Systems of agents </h3>

                <p> Multiagent Systems (MAS): </p>
                <p class="fragment"> systems or "organizations" of autonomous
                    agents, possibly <strong>distributed</strong> over multiple computers and/or in an environment, possibly belonging
                    to different stakeholders / organization (federated / open systems), collaborating and/or competing
                    to, respectively, achieve a shared global goal or maximise their own utility, possibly <span class="fragment highlight-green">interacting</span>
                    with a computational or physical <span class="fragment highlight-green">environment</span> (that could also mediate interactions) </p>

            </section>
            <section>

                <h3> Why? </h3>

                <small><ul class="fragment">
                    <li> A single agent has <strong>finite rationality</strong>, that is, a limit on the amount of knowledge he can rationally
                        handle in a given time </li>
                    <li> <strong>Distribution</strong>: several problems are intrinsically distributed, in that knowledge
                        can be acquired only <em>locally</em> </li>
                    <li> Interactions between <em>personal agents</em>: many problems requires components of different
                        stakeholders / organizations to interact, so the problem is intrinsically composed of multiple agents </li>
                    <li> Modelling for real-world organizations: software systems devoted to support the work of
                        some human organization should somehow "mimic" the structure of the real-world organizations </li>
                </ul></small>

            </section>
            <section>

                <p> MAS are "paradigmatic" of modern distributed systems: </p>
                <p class="fragment"> made up of decentralized <span class="fragment highlight-green">autonomous</span> components (sensors, peers, mobile devices, etc.) <span class="fragment highlight-green">interacting</span> with each
                    other in complex ways (P2P networks, MANETs, pervasive computing environments) and <span class="fragment highlight-green">situated</span> in
                some environment, computational or physical </p>

            </section>
            <section>

                <h3> Applications </h3>

                <small><ul>
                    <li> Trading and e-commerce: auctions, stock market, b2b, ... </li>
                    <li> Control of physical Processes: manufacturing pipeline, home/office automation, traffic control, ... </li>
                    <li> Shared resources management: cellular networks, power grid, computing, ... </li>
                    <li> Workflow Management: scientific research, administrative work, ... </li>
                    <li> Simulation of complex processes: biological systems, social sciences, games, ... </li>
                    <li> Optimization: any operational research domain, such as logistics, transportation, ... </li>
                </ul></small>

            </section>
            <section>
                <h3> Software engineering perspective </h3>

                <ul>
                    <p> In a MAS, agents participate by providing the capability of
                        achieving a goal in autonomy (vs. objects/services offering interfaces) </p>
                    <small><ul>
                        <li class="fragment"> the execution of an agent is autonomous, subject to its own <strong>internal decision</strong> (vs. objects/services being invoked) </li>
                        <li class="fragment"> agents interact in many complex ways, such as negotiation, auction, stigmergy, ... (vs. objects/services request/response) </li>
                        <li class="fragment"> agents interact because it is beneficial to themselves or the system (vs. objects/service are obligated to) </li>
                    </ul></small>
                </ul>

            </section>
            <section>

                <h3> Artificial intelligence perspective </h3>

                <ul>
                    <li> Different agents may have either conflicting goals, or individual goals contributing
                    to a systemic one, or a shared goal </li>
                    <li class="fragment"> In any case, the actions of an agent may <strong>influence</strong> other agents possibility of achieving their own goal </li>
                    <li class="fragment"> <em>Distributed decision making</em>:
                        <small><ul>
                            <li> <span class="fragment highlight-green">strategy</span>: how to behave taking into account what others could do? (competition, cooperation, ...) </li>
                            <li> <span class="fragment highlight-green">protocol</span>: what means to use to interact? (negotiation, argumentation, stigmergy, ...) </li>
                        </ul></small>
                    </li>
                </ul>

            </section>
            <section>

                <h3> Strategic thinking </h3>

                <ul>
                    <p> Assume we have: </p>
                    <ul class="fragment">
                        <li> agents $Ag = {i, j, ..., k}$ </li>
                        <li> each with its own goal $G_{Ag}$ </li>
                        <li> each with its own pool of actions $Ac_{Ag} = {a_{1}, a_{2}, ..., a_{n}}$ </li>
                    </ul>
                    <p class="fragment"> <span class="fragment highlight-red">How should agents decide which action to carry out (their <em>strategy</em>),
                        assuming they <strong>cannot communicate</strong>?</span> </p>
                </ul>

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Game theory:<br/>individual stance </h2>

            </section>
            <section>

                <h3> Game theory </h3>

                <p> To reply we need <span class="fragment highlight-green">game theory</span>:</p>
                <p class="fragment"> <em>"analysis of strategies for dealing with
                    <strong>competitive</strong> situations where the outcome of a participant's choice of action <strong>depends</strong>
                    critically on the actions of other participants"</em> </p>
                <p class="fragment"> Notice that agents <em>influence</em> each other even if they do not communicate, as
                    long as they act within a <strong>shared environment</strong>! </p>

            </section>
            <section>

                <h3> Rationality </h3>

                <ul>
                    <p> Agents are <strong>rational</strong> (recall Newell's principle?): </p>
                    <ul>
                        <li class="fragment"> given $Ac$ and $G$ they will attempt the actions that maximise the possibility to achieve $G$ </li>
                        <li class="fragment"> to do so they should be able to evaluate the outcome $\omega$ of an action
                            (e.g. success or failure) and the <strong>utility</strong> $u$ that such action brings towards achievement of $G$ </li>
                    </ul>
                    <p class="fragment"> The behaviour of rational agents of favouring actions that maximise $u$ is called <strong>preference</strong> </p>
                </ul>

            </section>
            <section>

                <ul>
                    <p> <strong>Utility functions</strong> are used to model preferences: $$u_{i} = \Omega \rightarrow \mathbb{R} \; \text{where} \; \Omega = {\omega_{1}, ..., \omega_{M}}$$ </p>
                    <p class="fragment"> Utility functions enable to define <em>preference orderings</em> over outcomes
                        (hence, actions): $$\omega >=_i \omega' \; \text{means} \; u_i(\omega) >= u_i(\omega')$$ as for different
                        agents $u$ and preferences may vary </p>
                </ul>

            </section>
            <section>

                <img class="stretch" data-src="utility-curve.png">

            </section>
            <section>

                <h3> Utility and adaptation </h3>

                <p> Utility is one of the many things that can work as a driver to adaptation: the lower the utility of an action,
                    the greater the chance for the agent to change action / behaviour / goal... </p>
                <p class="fragment"> As utility of an action may vary dynamically based on context (environment),
                adaptation of the agent may act and vary dynamically, too </p>

            </section>
            <section>

                <h3> Multiagent encounters (or, games) </h3>

                <p> If multiple agents either act (almost) <em>simultaneously</em> or do not have means to
                    <em>observe</em> each other actions, the outcome of their behaviours will still be some combination of each
                        individual outcome </p>
                <!--<p class="fragment"> A <em>state transformer function</em> models such combined outcome on the shared environment: $$\tau = Ac_i \times Ac_j \times... \rightarrow \Omega$$ </p>-->

            </section>
            <!--<section>

                <ul>
                    <p> E.g. considering both agents $i$ and $j$ to only have actions $a_1$ and $a_2$ available $$\tau(Ac_{i} = {a_1, a_2}, Ac_{j} = {a_1, a_2})$$ </p>
                    <small><ol>
                        <li class="fragment"> if $\tau(a_2,a_2) = \omega_1$ and $\tau(a_2,a_1) = \omega_2$ and $\tau(a_1,a_2) = \omega_3$ and $\tau(a_1,a_1) = \omega_4$ then the environment is sensitive to both agents </li>
                        <li class="fragment"> if $\tau(a_2,a_2) = \omega_1$ and $\tau(a_2,a_1) = \omega_1$ and $\tau(a_1,a_2) = \omega_1$ and $\tau(a_1,a_1) = \omega_1$ then the environment is sensitive to none </li>
                        <li class="fragment"> if $\tau(a_2,a_2) = \omega_1$ and $\tau(a_2,a_1) = \omega_2$ and $\tau(a_1,a_2) = \omega_1$ and $\tau(a_1,a_1) = \omega_2$ then the environment is sensitive only to agent $j$ </li>
                    </ol></small>
                </ul>

            </section>
            <section>

                <ul>
                    <p> Such state transformer functions let us find <strong>preferences</strong>, hence define a <span class="fragment highlight-green">rational choice</span> </p>
                    <small><p class="fragment"> Suppose to be in case 1 from previous examples, with utility functions:
                        $$u_i(\omega_1) = 1, u_i(\omega_2) = 1, u_i(\omega_3) = 4, u_i(\omega_4) = 4$$
                        $$u_j(\omega_1) = 1, u_i(\omega_2) = 4, u_i(\omega_3) = 1, u_i(\omega_4) = 4$$ </p>
                    <p class="fragment"> Then, agent $i$ preferences are: $$( (a_1,a_1) = \omega_4 \geq_i (a_1,a_2) = \omega_3 ) \gt_i ( (a_2,a_1) = \omega_2 \geq_i (a_2,a_2) = \omega_1 )$$ </p>
                    <p class="fragment"> Thus, agent $i$ rational choice is $a_1$: every outcome where it does $a_1$ are better than those where he chooses $a_2$, <strong>independently</strong> of what $j$ does </p></small>
                </ul>

            </section>-->
            <section>

                <h3> Dominant strategies </h3>

                <ul>
                    <p> The criteria according to which the choice of actions happens reflect the agent <strong>strategy</strong> (e.g. rational vs random) </p>
                    <ul>
                        <li class="fragment"> a strategy $s_1$ is said <strong>dominant</strong> with respect to $s_2$ if $s_1$ is always preferred to $s_2$ for every possible outcome </li>
                        <li class="fragment"> <em>a rational agent will never play a dominated strategy!</em> </li>
                        <li class="fragment"> if there is only one non-dominated strategy, that's the trivial strategy to adopt... </li>
                    </ul>
                </ul>

            </section>
            <section>

                <h3> Zero-sum games </h3>

                <p> In <strong>zero-sum</strong> games the utilities add up to $0$: $$u_i(\omega) + u_j(\omega) = 0 \; \forall \omega \in \Omega$$ </p>
                <p class="fragment"> Zero-sum games are <em>strictly competitive</em>: no agent can gain something if no other loses something (every win-lose game basically) </p>
                <p class="fragment"> <em>In these games there is no rational choice without information about other players' strategies!</em> </p>

            </section>
            <section>

                <h3> Rock-paper-scissors </h3>

                <img data-src="zero-sum.png">

            </section>
            <section>

                <h3> Non zero-sum games </h3>

                <p> Real-life situations are usually non zero-sum: there is always some "compromise" </p>
                <p class="fragment"> Check out <a href="https://youtu.be/LJS7Igvk6ZM"> this scene </a> from the movie "A beautiful mind" </p>

            </section>
            <section>

                <ul>
                    <p> <strong>Nash equilibrium</strong>: each player's predicted strategy is the
                        best response to the <em>predicted</em> strategies of other players </p>
                    <p class="fragment"> In other words, two strategies $s_1$ and $s_2$ are in Nash equilibrium if: </p>
                    <ul>
                        <li class="fragment"> under the assumption that agent $i$ plays $s_1$, agent $j$ can do no better than play $s_2$ </li>
                        <li class="fragment"> AND under the assumption that agent $j$ plays $s_2$, agent $i$ can do no better than play $s_1$ </li>
                    </ul>
                </ul>

            </section>
            <section>

                <p> Unfortunately there are both games with no Nash equilibrium (zero sum ones)
                    and with more than one Nash equilibrium (hence no trivial strategy to adopt)</p>
                <p class="fragment"> In games where there are common dominant strategies, they represent a Nash equilibrium </p>

            </section>
            <section>

                <h3> Prisoner's dilemma </h3>

                <ul>
                    <p> Two men are collectively charged with a crime and held in separate cells, with <em>no way of
                        meeting or communicating</em> </p>
                    <small><ul>
                        <li class="fragment"> if one confesses (action: $defect$) and the other does not (action: $coop$), the confessor will be
                            freed, and the other will be jailed for 3 years </li>
                        <li class="fragment"> if both confess, then each will be jailed for 2 years </li>
                        <li class="fragment"> if none confess, then each will be jailed for 1 year </li>
                    </ul></small>
                    <p class="fragment"> We can measure utilities in term of, e.g., years of prison
                        saved over the case of 4 years of prison (anything else could be ok) </p>
                </ul>

            </section>
            <section>

                <h3> Prisoners' payoff matrix </h3>

                <img data-src="prisoner.png">

            </section>
            <section>

                <ul>
                <p> The <strong>individual rational action</strong> is defect: it guarantees a payoff of
                    no worse than 2, whereas cooperating guarantees a payoff of at most 1 </p>
                    <ul>
                        <li class="fragment"> so defect-defect is the Nash Equilibrium (best strategy): both agents defect, and
                            get payoff = 2 </li>
                        <li class="fragment"> <span class="fragment highlight-red">but intuition says they should both cooperate and each get payoff of 3!</span> </li>
                    </ul>
                </ul>

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Game theory:<br/>collective stance </h2>

            </section>
            <section>

                <h3> What is missing? </h3>

                <p> This apparent paradox is the fundamental problem
                    of <strong>multiagent interactions</strong>: it <em>appears</em> to imply that cooperation will not occur in
                    societies of self-interested agents... </p>
                <li class="fragment"> <span class="fragment highlight-red">Can we make agents aware that <strong>cooperation</strong> is the best strategy?</span> </li>
                <li class="fragment"> <em>Notice that the prisoner's dilemma is ubiquitous in real-world problems!</em> </li>

            </section>
            <section>

                <ul>
                    <p> A possible answer: play the game <strong>repeatedly</strong> </p>
                    <ul>
                        <li class="fragment"> the incentive to defect evaporates (e.g. due to reputation, fear of retaliation, ...) </li>
                        <li class="fragment"> cooperation is the rational choice in the <em>indefinitely</em> repeated prisoner's dilemma! </li>
                    </ul>
                </ul>

            </section>
            <section>

                <h3> Axelrod's tournament </h3>

                <p> Suppose you play iterated prisoner's dilemma against a set of opponents: what
                    strategy should you choose, so as to maximize your overall (long-term) payoff? </p>
                <p class="fragment"> Axelrod (1984) investigated this problem, with a computer tournament for programs
                    playing the prisoner's dilemma: many different agents using different strategies, interacting hundreds
                    of times with other agents </p>

            </section>
            <section>

                <ul>
                    <p> Strategies: </p>
                    <ul class="fragment">
                        <li> ALLD: "Always defect" (hawk strategy) </li>
                        <li> TIT-FOR-TAT: on round $t_0$ cooperate, then at each $t_{i \gt 0}$ do what opponent
                        did on $t_{i-1}$ </li>
                        <li> TESTER: on 1st round defect, then if opponent retaliated start playing TIT-FOR-TAT,
                            otherwise intersperse cooperation and defection </li>
                        <li> JOSS: as TIT-FOR-TAT, but periodically defect </li>
                    </ul>
                </ul>

            </section>
            <section>

                <ul>
                    <p> In the long run, TIT-FOR-TAT is best strategy: <strong>cooperation</strong> wins! </p>
                    <p class="fragment"> Emerging "rules": </p>
                    <ul class="fragment">
                        <li> don’t be envious: don't play as if it were zero sum </li>
                        <li> be nice: start by cooperating, and always reciprocate cooperation immediately </li>
                        <li> not too nice: always punish defection immediately, but then don't overdue it </li>
                    </ul>
                </ul>

            </section>
            <section>
                <h3> Towards multiagent learning </h3>

                <p> Axelrod Tournament shows that a group of agents can change behaviour (i.e.,
                    strategy), to eventually <strong>learn</strong> what is the most suitable way of behaving to maximize own
                    success or/while maximize overall success of the group </p>
                <p class="fragment">A whole research topic on its own: <strong>Multi-agent Reinforcement Learning</strong></p>

            </section>
            <section>

                <h3> Learning $\approx$ adaptiveness? </h3>

                <ul>
                    <p> Long story short: no </p>
                    <small><ul class="fragment">
                        <li> the former implies acquiring new knowledge from past experience, to be used in future situations </li>
                        <li> the latter implies changing behaviour upon a change in context of action (can be purely <em>reactive</em>) </li>
                    </ul></small>
                    <p class="fragment"> However </p>
                    <small><ul class="fragment">
                        <li> learning may lead to adaptation, e.g. when new knowledge affects the utility function of an agent </li>
                        <li> agents may <em>learn how to adapt</em> (e.g. by monitoring outcomes of past adaptation actions) </li>
                    </ul></small>
                </ul>

            </section>
        </section>
        <section data-transition="convex">
            <!--<section data-transition="zoom">

                <h3> Multiagent systems </h3>

                <ul>
                    <li class="fragment"> Autonomous individuals vs. autonomous systems (FZ: slide 20 CAS) </li>
                    <li class="fragment"> FZ: NetLogo overview and example (?) </li>
                </ul>

            </section>-->
            <section data-transition="zoom">

                <h2> Multiagent interactions </h2>

            </section>
            <section>

                <h3> Forms of interaction </h3>

                <ul>
                    <p> Differently from game theory, let's now assume that <em>agents can interact</em>: </p>
                    <small><ul>
                        <li class="fragment"> <strong>direct</strong> interactions: agents directly communicate with each other by exchanging messages </li>
                        <li class="fragment"> <strong>indirect</strong> (or stigmergic) interactions: agents interact indirectly by accessing
                            an <strong>environment</strong> in which they operate (physical, computational, or mixed) </li>
                    </ul></small>
                    <p class="fragment"> As they have a mean to affect each other's actions and beliefs, they can strategically act
                        based on what the other agents do (<strong>observation</strong>), and agree on common courses of actions </p>
                </ul>

            </section>
            <section>

                <ul>
                    <p> Interactions may imply: </p>
                    <ul class="fragment">
                        <li> <strong>communication</strong>: explicit exchange of beliefs, desires, intentions, ... </li>
                        <li> <strong>synchronization</strong>: ordering of concurrent actions </li>
                        <li> <strong>coordination</strong>: organizing a shared plan of actions </li>
                    </ul>
                </ul>

            </section>
            <section>

                <h3> Types of interaction </h3>

                <ul>
                    <li> <strong>Collaborative</strong>: agents cooperate towards the achievement of some common
                        application goals, and <em>trust</em> each other because they know they work towards the <em>same goal</em> </li>
                    <li class="fragment"> <strong>Competitive</strong>: agents are forced to interact to achieve <em>individual goals</em>, but
                        they don't have to trust each other </li>
                    <li class="fragment"> <strong>Collaborative competition</strong>: agents compete but for the sake of achieving
                        some global goal, hence they trust each other </li>
                </ul>

            </section>
            <section>

                <h3> MAS vs. distributed algorithms </h3>

                <p> In distributed algorithms, too, there is need of reaching agreement on how to
                    act or on a common perspective of the world (leader election, mutual exclusion, validation of a
                    blockchain transaction, etc.) </p>
                <p class="fragment"> However, there is no concept such as "goal", "utility" of actions, etc.: either
                    the algorithms works, or fails </p>

            </section>
            <section>

                <h3> Interaction protocol </h3>

                <ul>
                    <p> Defines the <em>rules</em> of the encounter between agents: the set of available interaction
                        actions, their <strong>dependencies</strong>, how they affect the "state of the world", etc. </p>
                    <small><ul>
                        <li class="fragment"> direct communication: sequence of messages between agents aimed at discussing
                            how to reach agreement </li>
                        <li class="fragment"> stigmergic interactions: the form and structure of the "signs" (traces) in the environment,
                            how and when agents should leave signs, and how they should react to these </li>
                    </ul></small>
                    <p class="fragment"> In this context, agents' strategy regards freedom in deciding what to do at
                        each step of the protocol amongst admissible actions </p>
                </ul>

            </section>
            <section>

                <ul>
                    <p> E.g. "battle of the sexes": </p>
                    <ol class="fragment">
                        <li> MAN: "Football!" </li>
                        <li> WOMAN: "Movie!" </li>
                        <li> MAN: "This time Football next time Movie" </li>
                        <li> WOMAN: "No, this time movie and next time Football" </li>
                        <li> MAN: "No, This time Football next TWO times Movie" </li>
                        <li> WOMAN: "OK" </li>
                        <li> MAN: "OK" </li>
                    </ol>
                </ul>

            </section>
            <section>

                <h3> Protocol (interaction space) design </h3>

                <ul>
                    <p> Designing an interaction protocol implies devising a sequence of interactions
                        with desired properties </p>
                    <small><ol>
                        <li class="fragment"> MAN: Proposal </li>
                        <li class="fragment"> WOMAN: Proposal </li>
                        <li class="fragment"> if action differs: </li>
                        <ol>
                            <li class="fragment"> MAN: agree or counter-proposal </li>
                            <li class="fragment"> WOMAN: agree or counter-proposal </li>
                        </ol>
                        <li class="fragment"> while (agree < 2) </li>
                        <ol>
                            <li class="fragment"> MAN: agree or counter-proposal with increased WOMAN utility </li>
                            <li class="fragment"> WOMAN: agree or counter-proposal with increased MAN utility </li>
                        </ol>
                    </ol></small>
                    <p class="fragment"> Strategy: as soon as the counter-proposal reaches a sufficiently high utility,
                        agree </p>
                </ul>

            </section>
            <section>

                <h3> Direct interaction: negotiation </h3>

                <ul>
                    <p> <em>"Negotiation is an economically-inspired form of <strong>distributed decision making</strong> where two or more
                        partners jointly search a space of possible solutions to reach a common consensus"</em> (P. Maes) </p>
                    <ul>
                        <li class="fragment"> Collaborative --> <span class="fragment highlight-green">ContractNet</span>, argumentation </li>
                        <li class="fragment"> Competitive --> <span class="fragment highlight-green">Auctions</span> </li>
                    </ul>
                    <p class="fragment"> Applications </p>
                    <small><ul class="fragment">
                        <li> Cellular networks </li>
                        <li> Manufacturing </li>
                        <li> Supply chain </li>
                    </ul></small>
                </ul>

            </section>
            <section>

                <h3> ContractNet </h3>

                <small><ol>
                    <li> An agent (initiator) needs help from others to carry out a set of tasks (because it cannot
                        or prefer not to do them itself), hence informs other agents about these tasks (type,
                        description, requirements, deadline, etc) and asks them to PROPOSE to perform them </li>
                    <li class="fragment"> Other agents (responders) place "bids" specifying how they would be able to carry out
                        such task (in what time, with what efficiency and accuracy, etc) </li>
                    <li class="fragment"> The initiator then <strong>decides</strong> how to assign each task to the "best" responder (according to
                        whatever criteria / strategy) </li>
                </ol></small>

            </section>
            <section>

                <h3> JADE ContractNet: initiator </h3>

                <pre class="java"><code>
public class Initiator extends Agent {

    @Override
    protected void setup() {
        this.addBehaviour(new ContractNetInitiator(this, new ACLMessage(ACLMessage.CFP)) {
            @Override
            protected Vector prepareCfps(ACLMessage cfp) {
                /* prepare ACL message Call for Proposals */
                return super.prepareCfps(cfp);
            }

            @Override
            protected void handlePropose(ACLMessage propose, Vector acceptances) {
                /* handle proposal coming from responders */
                super.handlePropose(propose, acceptances);
            }

            @Override
            protected void handleRefuse(ACLMessage refuse) {
                /* handle refusals coming from responders */
                super.handleRefuse(refuse);
            }

            @Override
            protected void handleAllResponses(Vector responses, Vector acceptances) {
                /* when all replies to CFP have been collected, do something (select winner) */
                super.handleAllResponses(responses, acceptances);
            }

            @Override
            protected void handleInform(ACLMessage inform) {
                /* handle confirmations of tasks carried out coming from responders */
                super.handleInform(inform);
            }

        });
    }

}
                </code></pre>

                <p><small> Get the code here: <a href="https://github.com/smarianimore/phdcourse-2020-jade">https://github.com/smarianimore/phdcourse-2020-jade</a> </small></p>

            </section>
            <section>

                <h3> JADE ContractNet: responder </h3>

                <pre class="java"><code>
public class Responder extends Agent {

    @Override
    protected void setup() {
        MessageTemplate template = MessageTemplate.and(
                MessageTemplate.MatchProtocol(FIPANames.InteractionProtocol.FIPA_CONTRACT_NET),
                MessageTemplate.MatchPerformative(ACLMessage.CFP) );
        this.addBehaviour(new ContractNetResponder(this, template) {
            @Override
            protected ACLMessage handleCfp(ACLMessage cfp) throws RefuseException, FailureException, NotUnderstoodException {
                /* react to reception of call for proposals */
                return super.handleCfp(cfp);
            }

            @Override
            protected ACLMessage handleAcceptProposal(ACLMessage cfp, ACLMessage propose, ACLMessage accept) throws FailureException {
                /* react to acceptance of own proposal (do the task, then inform result) */
                return super.handleAcceptProposal(cfp, propose, accept);
            }
        });
    }

}
                </code></pre>

                <p><small> Get the code here: <a href="https://github.com/smarianimore/phdcourse-2020-jade">https://github.com/smarianimore/phdcourse-2020-jade</a> </small></p>

            </section>
            <section>

                <h3> Auctions </h3>

                <p> When agents have competing interests and no interest
                    in cooperating, the only solution for cooperation is to "pay" the actions/tasks/resources
                    that agents provide to others </p>
                <p class="fragment"> Auctions are the negotiation mechanisms that determines
                    the <strong>values</strong> of a good/resource/actions to be "sold" by an
                    offering (seller) agents to buyer agent(s) </p>

            </section>
            <section>

                <h3> Indirect (mediated) interaction </h3>

                <ul>
                    <p> All the form of direct interaction seen can be replicated as indirect ones,
                        based on some sort of <strong>environment mediation</strong> </p>
                    <ul>
                        <li class="fragment"> <span class="fragment highlight-green">Tuple spaces</span>: shared repositories of data with access mechanisms embedding
                        synchronisation of interactions </li>
                        <li class="fragment"> <span class="fragment highlight-green">Pheromone-based coordination</span>: inspired by collective behaviour of social insects
                        such as ants </li>
                        <li class="fragment"> <span class="fragment highlight-green">Field-based coordination</span>: inspired by gravitational waves influencing how
                        objects behave </li>
                    </ul>
                </ul>

            </section>
            <section>

                <h3> The tuple space model </h3>

                <img class="stretch" data-src="ts.png">

            </section>
            <section>

                <h3> <a href="http://apice.unibo.it/xwiki/bin/view/TuCSoN/">TuCSoN</a>: greeting agents </h3>

                <p><small> Get the code here: <a href="https://gitlab.com/pika-lab/courses/ds/aa1920/lab-02">https://gitlab.com/pika-lab/courses/ds/aa1920/lab-02</a> </small></p>

                <pre class="java"><code>
public class SteAgent extends AbstractTucsonAgent {

    public static void main(String[] args) throws TucsonInvalidAgentIdException {
        new SteAgent().go();
    }

    public SteAgent() throws TucsonInvalidAgentIdException {
        super("ste_agent");
    }

    @Override
    protected void main() {
        try {
            SynchACC helloOps = getContext();
            final TucsonTupleCentreId defaultTC = new TucsonTupleCentreId("default", "localhost", "20504");
            final LogicTuple helloTuple = LogicTuple.parse("msg(gio,hello)");
            final LogicTuple steTemplate = LogicTuple.parse("msg(ste,_)");

            helloOps.out(defaultTC, helloTuple, Long.MAX_VALUE);
            helloOps.in(defaultTC, steTemplate, Long.MAX_VALUE);

        } catch (final OperationTimeOutException | TucsonInvalidTupleCentreIdException | InvalidLogicTupleException | TucsonOperationNotPossibleException | UnreachableNodeException e) {
            e.printStackTrace();
        }
    }

}
                </code></pre>

                <p><small>(kindly provided by <a href="https://about.me/gciatto">Giovanni Ciatto</a>)</small></p>

            </section>
            <section>

                <p><small> Get the code here: <a href="https://gitlab.com/pika-lab/courses/ds/aa1920/lab-02">https://gitlab.com/pika-lab/courses/ds/aa1920/lab-02</a> </small></p>

                <pre class="java"><code>
public class GioAgent extends AbstractTucsonAgent {

    public static void main(String[] args) throws TucsonInvalidAgentIdException {
        new GioAgent().go();
    }

    public GioAgent() throws TucsonInvalidAgentIdException {
        super("gio_agent");
    }

   @Override
    protected void main() {
        try {
            SynchACC helloOps = getContext();
            final TucsonTupleCentreId defaultTC = new TucsonTupleCentreId("default", "localhost", "20504");
            final LogicTuple worldTuple = LogicTuple.parse("msg(ste,world)");
            final LogicTuple gioTemplate = LogicTuple.parse("msg(gio,_)");

            helloOps.in(defaultTC, gioTemplate, Long.MAX_VALUE);
            helloOps.out(defaultTC, worldTuple, Long.MAX_VALUE);

        } catch (final OperationTimeOutException | TucsonInvalidTupleCentreIdException | InvalidLogicTupleException | TucsonOperationNotPossibleException | UnreachableNodeException e) {
            e.printStackTrace();
        }
    }

}
                </code></pre>

                <p><small>(kindly provided by <a href="https://about.me/gciatto">Giovanni Ciatto</a>)</small></p>

            </section>
            <section>

                <h3> Interactions enable adaptiveness </h3>

                <p> In several cases, <em>ensembles</em> of very simple, purely "reactive"
                    components, may globally exhibit adaptive behaviours, <strong>as a system</strong> (e.g. single cell vs. organism) </p>
                <p class="fragment"> In these cases, adaptiveness is a capability
                    that <em>emerges</em> not from individual behaviour in isolation, but from individuals' <strong>interactions</strong> (nature, medium, rate, ...) </p>

            </section>
            <section>

                <h3> NetLogo </h3>

                <ul>
                    <p> Programmable modeling environment for <strong>simulating complex systems</strong> ($\approx$ many individuals, many interactions, emergent behaviour) </p>
                    <small><ul>
                        <li class="fragment"> can give instructions to hundreds or thousands of independent "agents", all
                            operating in parallel </li>
                        <li class="fragment"> makes it possible to explore the connection between the <strong>micro-level</strong> (individual behaviour) and the <strong>macro-level</strong> (global patterns) </li>
                    </ul></small>
                    <p class="fragment"><small> Either download or use online: <a href="https://ccl.northwestern.edu/netlogo/">https://ccl.northwestern.edu/netlogo/</a> </small></p>
                </ul>

            </section>
            <section>

                <h3> NetLogo: pheromones </h3>

                <img class="stretch" data-src="ants.png">

                <p> <a href="http://www.netlogoweb.org/launch#http://www.netlogoweb.org/assets/modelslib/Sample%20Models/Biology/Ants.nlogo">Try it online!</a> </p>

            </section>
            <section>

                <h3> NetLogo: fields (flocking) </h3>

                <img class="stretch" data-src="flocking.png">

                <p> <a href="http://www.netlogoweb.org/launch#http://www.netlogoweb.org/assets/modelslib/Alternative%20Visualizations/Flocking%20-%20Alternative%20Visualizations.nlogo">Try it online!</a> </p>

            </section>
            <section>

                <h3> NetLogo: fields (predation) </h3>

                <img class="stretch" data-src="predation.png">

                <p> <a href="http://www.netlogoweb.org/launch#http://www.netlogoweb.org/assets/modelslib/Sample%20Models/Biology/Wolf%20Sheep%20Predation.nlogo">Try it online!</a> </p>

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Conclusion </h2>

            </section>
            <section>

                <h3> Main takeways </h3>

                <p> Autonomous systems already are "among us" </p>
                <p class="fragment"><small> <strong>Individual</strong> behaviour is half of the story: <strong>interactions</strong>, hence collective behaviour, is equally relevant (or even more) </small></p>
                <p class="fragment"><small> Many ways to design <strong>individual autonomy</strong>: BDI, game theory, <em>learning</em> (not explored here), ... </small></p>
                <p class="fragment"><small> Many ways to design <strong>system autonomy</strong>: interaction protocols, mediated interaction, game theory, ... </small></p>
                <p class="fragment"> Engineering of <em>autonomous yet predictable, controllable, and understandable systems</em> is an open challenge, still </p>

            </section>
            <section>

                <h3> Personal interests </h3>

                <ul>
                    <li> How to best <span class="fragment highlight-red">coordinate</span> agents in a multiagent system? </li>
                    <li class="fragment"> Can effective and efficient communication for coordination <span class="fragment highlight-red">be learnt</span>? </li>
                    <li class="fragment"> Applications to Internet of Things, supply chain, manufacturing, swarm robotics, ... </li>
                </ul>

            </section>
            <section>

                <h3> Coordination </h3>

                <ul>
                    <li> <a href="https://ieeexplore.ieee.org/document/8114170">An Argumentation-Based Perspective Over the Social IoT</a> </li>
                    <li> <a href="https://smarianimore.github.io/2022/11/15/student-prima2022.html">Cooperative Driving at Intersections through Agent-based Argumentation</a> </li>
                </ul>

            </section>
            <section>

                <h3> Learning </h3>

                <ul>
                    <li> <a href="https://smarianimore.github.io/2023/09/19/paper-acsos2023.html">Learning Stigmergic Communication for Self-organising Coordination</a> </li>
                    <li> <a href="https://smarianimore.github.io/2023/07/19/paams2023-update.html">Multi-agent Learning of Causal Networks in the Internet of Things</a> </li>
                </ul>

            </section>
        </section>
        <section data-transition="fade">
            <section data-autoslide="2000">

                <br/>
                <h2> Thanks </h2>
                <h2> for your attention </h2>
                <h4 class="fragment fade-in-then-semi-out"> Questions? </h4>
                <small><p class="fragment"> <a href="index-blockchain.html"> <em>(continue to part II)</em> </a> </p></small>
                <br/>
                <p><a href="https://smarianimore.github.io"> Stefano Mariani </a></p>
                <p><small><em> University of Modena and Reggio Emilia </em></small></p>

            </section>
            <!--<section data-autoslide="3000">

                <h3> References </h3>

            </section>-->
        </section>
    </div>
</div>

<link rel="stylesheet" href="lib/css/monokai.css">
<script src="js/reveal.js"></script>
<script src="plugin/math/math.js"></script>

<script>
    Reveal.initialize({
        autoSlide: 1000,
        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: 120,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        pdfSeparateFragments: false, // http://localhost:63342/reveal.js/?print-pdf/&showNotes=true
        dependencies: [
            {src: 'plugin/markdown/marked.js'},
            {src: 'plugin/markdown/markdown.js'},
            {src: 'plugin/zoom-js/zoom.js'},
            {src: 'plugin/notes/notes.js'},
            {src: 'plugin/highlight/highlight.js', async: true}
        ],
        plugins: [ RevealMath ],
        slideNumber: true
    });
</script>

</body>
</html>
