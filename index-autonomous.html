<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport">

    <title> Emerging IT Technologies: Blockchains and Autonomous Systems </title>
    <meta content="PhD course 'Emerging IT Technologies: Blockchains and Autonomous Systems' given by Stefano Mariani" name="description">
    <meta content="Stefano Mariani" name="author">

    <meta content="yes" name="apple-mobile-web-app-capable">
    <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">

    <link href="css/reset.css" rel="stylesheet">
    <link href="css/reveal.css" rel="stylesheet">
    <link href="css/theme/blood.css" rel="stylesheet">
    <!-- black, white, league, beige, sky, blood, night, serif, simple, solarized -->

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>

<body>
<div class="reveal">
    <div class="slides">
        <section data-autoslide="5000" data-transition="fade">

            <h2> Autonomous Systems: </h2>
            <br/>
            <h3> an overview </h3>
            <br/>
            <br/>
            <p><a href="http://stefanomariani.apice.unibo.it"> Stefano Mariani </a></p>
            <p><small><em> Università di Modena e Reggio Emilia </em></small></p>

        </section>
        <section data-transition="convex" data-autoslide="2000">

            <h3> Outline </h3>

            <ol>
                <li class="fragment"> On the notion of autonomy </li>
                <li class="fragment"> Agents </li>
                <li class="fragment"> Multiagent systems </li>
                <li class="fragment"> Game theory: individual stance </li>
                <li class="fragment"> Game theory: collective stance </li>
                <li class="fragment"> Multiagent interactions </li>
            </ol>

        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> On the notion of autonomy </h2>

            </section>
            <section>

                <h3> On the notion of autonomy </h3>

                <p class="fragment"> <span class="fragment highlight-red"> Why bother? </span> </p>
                <p class="fragment"> <span class="fragment highlight-green"> Social pressure</span>: unproductive mind work and non-qualified time
                        consuming activities might be delegated to artificial systems </p>

            </section>
            <section>

                <h3> Issues </h3>

                <p class="fragment"> <span class="fragment highlight-red">Who does what?</span> </p>
                <p class="fragment"> Mostly, this is no longer an issue: artificial system are
                    generally very welcome to do whatever we like </p>
                <p class="fragment"> <span class="fragment highlight-red">Who takes the decision?</span> </p>
                <p class="fragment"> Autonomy is at least about <strong>deliberation</strong> as
                    much as about <strong>action</strong> </p>
                <!--<li class="fragment"> <em>Executive vs. motivational autonomy</em> [Castelfranchi, 1995]: given a <strong>goal</strong>, an
                    "agent" is autonomous in achieving it by itself, vs. the agent's goals are somehow self-generated, not externally imposed </li>-->

            </section>
            <section>

                [pic hype cycles 2018-2020 AO]

            </section>
            <section>

                <h3> Research questions </h3>

                <p class="fragment"> Is there just one single notion of <strong>autonomy</strong>? </p>
                <p class="fragment"> How do we <em>model</em> autonomy in artificial / computational systems? </p>
                <p class="fragment"> How do we <em>engineer</em> autonomy in artificial / computational systems? </p>

            </section>
            <section>

                <h3> Autonomy in context </h3>

                <p> According to [Maturana and Varela, 1980]: </p>

                <p> <em>in the domain of systems biology,
                    autonomous systems acquire the property of specifying their own rules of behaviour, do not work
                    as mere transducers or functions for converting input instructions into output products, and are
                    themselves sources of their own activity</em> </p>

            </section>
            <section>

                <p> From [Unmanned Systems Integrated Roadmap FY 2011-2036]: </p>
                <p> <em>systems are self-directed toward a <strong>goal</strong> in that they do not require outside control, but rather are
                    governed by laws and strategies that direct their behavior</em> </p>
                <p class="fragment"> <em>These control algorithms are created and tested by teams of human operators and
                    software developers. However, if <em>machine learning</em>
                    is utilized, autonomous systems can develop modified strategies for themselves by which
                    they select their behavior</em> </p>

            </section>
            <section>

                <p> Various levels of autonomy in any system guide how much and how often
                    humans need to interact or intervene with the autonomous system: [pic slide 15 C3 AO]</p>

            </section>
            <section>

                <p> Level of <em>automation</em> for cars [SAE J3016, 2018]: [pic slide 21,22 C3 AO] </p>

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Agents </h2>

            </section>
            <section>

                <h3> Software engineering perspective </h3>

                <ul>
                    <p> An <strong>object</strong> (e.g. in Java) is: </p>
                        <small><ul>
                            <li> state (instance or state variables) </li>
                            <li> methods (operations) </li>
                        </ul></small>
                    <p class="fragment"> Actually, other than state and methods: </p>
                        <small><ul class="fragment">
                            <li> internal threads </li>
                            <li> event handling </li>
                            <li> messaging </li>
                            <li> access to contextual information </li>
                        </ul></small>
                    <p class="fragment"> <span class="fragment highlight-red">Is it still an object?</span> </p>
                </ul>

            </section>
            <section>

                <ul>
                    <p> The "grown-up" objects and services of modern adaptive software are </p>
                        <small><ul class="fragment">
                            <li> not purely functional (they do not simply answer to request of services but rather try to achieve an
                                objective, a goal) </li>
                            <li> capable of unsolicited execution (due to internal threads) </li>
                            <li> adaptive (they can dynamically acquire information and tune their behavior accordingly) </li>
                            <li> <em>situated</em> (access to contextual and environmental information) </li>
                            <li> <em>social</em> (they interact with each other either via messaging or via mediated
                                interactions via the environment) </li>
                        </ul></small>
                    <p class="fragment"> This is very close to the definition of <span class="fragment highlight-green">agents</span>... </p>
                </ul>

            </section>
            <section>

                <h3> Software agents </h3>

                <p> Autonomy related to <strong>decision making</strong>: </p>
                <p class="fragment"> centralised decision making, as in service-oriented and object-based applications, achieves global goal via design by <em>delegation of
                    control</em> </p>
                <p class="fragment"> distributed decision making, as in agent-based applications, achieves global goal via
                    design by <em>delegation of responsibility</em> (<strong>agents can say no</strong>) </p>

            </section>
            <section>

                <p> A software agent is a component that is:

                    <small><ul>
                        <li class="fragment"> <span class="fragment highlight-green">Goal-oriented</span>: designed and deployed to achieve a specific goal (or to perform a specific task) </li>
                        <li class="fragment"> <span class="fragment highlight-green">Autonomous</span>: capable of acting in autonomy towards the achievement of its specific goals, without being
                            subject to a globally controlled thread of control </li>
                        <li class="fragment"> <span class="fragment highlight-green">Situated</span>: executes in the context of a specific environment
                            (computational or physical), and is able act in that environment by sensing and
                            affecting (via sensors and actuators) </li>
                        <li class="fragment"> <span class="fragment highlight-green">Social</span>: can interact with other agents in a multiagent system </li>
                    </ul></small>

            </section>
            <section>

                <h3> Autonomy in software agents </h3>


                <p> Agents can decide to autonomously activate towards the pursuing of the
                    goal, without the need of any specific event or solicitation: </p>
                <p class="fragment"> <strong>proactivity</strong> is a sort of extreme expression of autonomy </p>

            </section>
            <section>

                <p> Clearly, it is not always black and white, as modern objects have features
                    that can make objects resemble agents </p>
                <p> In effect, several systems for "agent-oriented programming" can be considered simply as advanced
                    tools for object-oriented programming or for more dynamic forms of service-oriented programming </p>

            </section>
            <section>

                <h3> Artificial intelligence perspective </h3>

                <p> For many researchers, agents do not simply have to be goal-oriented,
                    autonomous, situated, but they have to be <em>"intelligent"</em>, that typically means they have to
                    integrate "artificial intelligence" tools: neural networks, logic-based reasoning, conversational
                    capabilities (interact via natural language), ...  </p>

            </section>
            <section>

                <p> Treating a program as if it is intelligent ("the program <em>wants</em> the input
                    in a different format") is called the <strong>intentional stance</strong>, and it's helpful to us as programmers
                    to think this way </p>
                <p class="fragment"> The intentional stance leads us to program agents at the knowledge level (Newell),
                    that means reasoning about programs in terms of: </p>
                <small><ul class="fragment">
                    <li> facts and <strong>beliefs</strong> (rather than variables and data) </li>
                    <li> goals and <strong>intentions</strong> (rather than functionalities and methods) </li>
                    <li> <strong>desires</strong> and preferences </li>
                </ul></small>

            </section>
            <section>

                <p> A possible definition of intelligence is: </p>
                <p class="fragment"> <em>"the capability to act purposefully towards the achievement of goals"</em> </p>
                <p class="fragment"> Hence for an agent to <em>be regarded (observed) as</em> being intelligent it is enough
                    simply to know how to achieve a given goal, which implies some sort of <strong>reasoning</strong> </p>

            </section>
            <section>

                <ul>
                    <p> E.g. a "smart" thermostat: </p>
                    <ul>
                        <li class="fragment"> Goal: keep temperature at 27° </li>
                        <li class="fragment"> Percept: temperature </li>
                        <li class="fragment"> Actions: increase / decrease temperature </li>
                        <li class="fragment"> Reasoning: <span><pre><code data-trim>
    if temp < 27:
        temp.increase()
    else if temp > 27:
        temp.decrease()
                            </code></pre></span></li>
                    </ul>
                    <p class="fragment"> Too simplistic, too low level of abstraction </p>
                </ul>

            </section>
            <section>

                <ul>
                    <p> Imagine a self-driving car: </p>
                    <ul>
                        <li class="fragment"> Goal: don't kill pedestrians </li>
                        <li class="fragment"> Percept: current <em>situation</em> (where are pedestrians, other cars, weather conditions, ...) </li>
                        <li class="fragment"> Actions: gas, brake, steer, ... </li>
                        <li class="fragment"> <span class="fragment highlight-red">Reasoning</span>: still if-then-else?? </li>
                    </ul>
                    <p class="fragment"> Obviously not: <strong>theoretical</strong> + <strong>practical reasoning</strong> </p>
                </ul>

            </section>
            <section>

                <h3> Intelligence as "observing reasoning" </h3>

                <ul>
                    <li class="fragment"> <span class="fragment highlight-green">Theoretical reasoning</span>: a process which affects <strong>beliefs</strong>, to expand / improve
                        knowledge about what's happening (<em>inference</em>) </li>
                    <li class="fragment"> <span class="fragment highlight-green">Practical reasoning</span>: weighing conflicting considerations for and against
                        competing options, given what the agent <strong>desires</strong> and it believes (<em>decision making</em>) </li>
                    <small><ul>
                        <li class="fragment"> <strong>Deliberation</strong>: decision about what goal to pursue (adopting <em>intentions</em>) </li>
                        <li class="fragment"> <strong>Means-ends reasoning</strong>: decision about how to achieve goal (carry out actions) </li>
                    </ul></small>
                </ul>

            </section>
            <section>

                <h3> On rationality </h3>

                <p> Newell's Principle of <span class="fragment highlight-green">rationality</span>: </p>
                <p class="fragment"> if an agent has the knowledge that an action will
                    lead to the accomplishment of one of its goals (or to the maximization of its <span class="fragment highlight-green">utility</span>), then it will
                    select that action (<strong>Game Theory</strong> and <strong>Decision Theory</strong>) </p>

            </section>
            <section>

                <ul>
                    <p> <strong>BDI</strong> is a very successful and general model to "think" at software agents: </p>
                    <p class="fragment"> The agent has: </p>
                        <ul>
                            <li class="fragment"> Beliefs (what it knows about the world) </li>
                            <li class="fragment"> Desires (what it desires to happen) </li>
                            <li class="fragment"> Intentions (what it wants to do to satisfy its desires) </li>
                        </ul>
                </ul>

            </section>
            <section>

                <h3> Agent architectures </h3>

                <p> What types of architectures can we conceive for agents [pics FZ 36-46]? </p>
                <small><ul>
                    <li class="fragment"> <strong>Reactive</strong> (or tropistic): "if-then-else" behaviour, simple and efficient, not very expressive, no real autonomy (agents act solely upon external stimuli) </li>
                    <li class="fragment"> Reactive with <strong>State</strong> (hysteretic): tracking the state of the world enables the agent to act in context, e.g. different actions with same stimulus </li>
                    <li class="fragment"> <strong>Goal-oriented</strong>: the agent plans its actions towards achievement of a desired state [image slide 29] </li>
                    <li class="fragment"> <strong>Utility-oriented</strong>: the agent aims at maximising a function of some parameter measuring what it gains from a given state [image slide 31] </li>
                </ul></small>
                </p>

            </section>
            <section>

                <p> JADE example ? </p>

            </section>
            <section>

                <p> Jason example ? </p>

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Multiagent systems </h2>

            </section>
            <section>

                <h3> Systems of agents </h3>

                <p> Multiagent Systems (MAS): </p>
                <p class="fragment"> systems or "organizations" of <em>interacting</em> autonomous
                    agents, possibly <em>distributed</em> over multiple computers and/or in an environment, possibly belonging
                    to different stakeholders / organization (federated / open systems), collaborating and/or competing
                    to, respectively, achieve a shared global goal or maximise their own utility, possibly <span class="fragment highlight-green">interacting</span>
                    with a computational or physical <span class="fragment highlight-green">environment</span> (that could also mediate interactions) </p>

            </section>
            <section>

                <h3> Why? </h3>

                <small><ul>
                    <li class="fragment"> A single agent has finite rationality, that is, a limit on the amount of knowledge he can rationally
                        handle in a given time </li>
                    <li class="fragment"> Distribution: several problems are intrinsically distributed, in that knowledge
                        can be acquired only <em>locally</em> </li>
                    <li class="fragment"> Interactions between <em>personal agents</em>: many problems requires components of different
                        stakeholders / organizations to interact, so the problem is intrinsically composed of multiple agents </li>
                    <li class="fragment"> Modelling for real-world organizations: software systems devoted to support the work of
                        some human organization should somehow "mimic" the structure of the real-world organizations </li>
                </ul></small>

            </section>
            <section>

                <p> MAS are "paradigmatic" of modern distributed systems: </p>
                <p class="fragment"> made up of decentralized <strong>autonomous</strong> components (sensors, peers, mobile devices, etc.) <strong>interacting</strong> with each
                    other in complex ways (P2P networks, MANETs, pervasive computing environments) and <strong>situated</strong> in
                some environment, computational or physical </p>

            </section>
            <section>

                <h3> Applications </h3>

                <ul>
                    <li> Trading and e-commerce: auctions, stock market, b2b, ... </li>
                    <li> Control of physical Processes: manufacturing pipeline, home/office automation, traffic control, ... </li>
                    <li> Shared resources management: cellular networks, power grid, computing, ... </li>
                    <li> Workflow Management: scientific research, administrative work, ... </li>
                    <li> Simulation of complex processes: biological systems, social sciences, games, ... </li>
                    <li> Optimization: any operational research domain, such as logistics, transportation, ... </li>
                </ul>

            </section>
            <section>
                <h3> Software engineering perspective </h3>

                <ul>
                    <p> In a MAS, agents participate by providing the capability of
                        achieving a goal in autonomy (vs. objects/services offering interfaces) </p>
                    <li class="fragment"> the execution of an agent is autonomous, subject to its own internal decision (vs. objects/services being invoked) </li>
                    <li class="fragment"> agents interact in many complex ways, such as negotiation, auction, stigmergy, ... (vs. objects/services request/response) </li>
                    <li class="fragment"> agents interact because it is beneficial to themselves or the system (vs. objects/service are obligated to) </li>
                </ul>

            </section>
            <section>

                <h3> Artificial intelligence perspective </h3>

                <ul>
                    <li> Different agents may have either conflicting goals, or individual goals contributing
                    to a systemic one, or a shared goal </li>
                    <li class="fragment"> In any case, the actions of an agent may influence other agents possibility of achieving their own goal </li>
                    <li class="fragment"> <em>Distributed decision making</em>: </li>
                    <small><ul class="fragment">
                        <li> <span class="fragment highlight-green">strategy</span>: how to behave taking into account what others could do? (competition, cooperation, ...) </li>
                        <li> <span class="fragment highlight-green">protocol</span>: what means to use to interact? (negotiation, argumentation, stigmergy, ...) </li>
                    </ul></small>
                </ul>

            </section>
            <section>

                <h3> Strategic thinking </h3>

                <ul>
                    <p> Assume we have: </p>
                    <ul class="fragment">
                        <li> agents $Ag = {i, j, ..., k}$ </li>
                        <li> each with its own goal $G_{Ag}$ </li>
                        <li> each with its own pool of actions $Ac_{Ag} = {a_{1}, a_{2}, ..., a_{n}}$ </li>
                    </ul>
                    <p class="fragment"> How should agents decide which action to carry out (their <em>strategy</em>),
                        assuming they <strong>cannot communicate</strong>? </p>
                </ul>

            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Game theory: individual stance </h2>

            </section>
            <section>

                <h3> Game theory </h3>

                <p> To reply we need <span class="fragment highlight-green">game theory</span>:</p>
                <p class="fragment"> <em>"analysis of strategies for dealing with
                    <strong>competitive</strong> situations where the outcome of a participant's choice of action <strong>depends</strong>
                    critically on the actions of other participants"</em> </p>
                <p class="fragment"> Notice that agents influence each other even if they do not communicate, as
                    long as they act within a <em>shared environment</em>! </p>

            </section>
            <section>

                <h3> Rationality </h3>

                <ul>
                    <p> Agents are <strong>rational</strong> (remember Newell's principle?): </p>
                    <ul>
                        <li class="fragment"> given $Ac$ and $G$ they will attempt the actions that maximise the possibility to achieve $G$ </li>
                        <li class="fragment"> to do so they should be able to evaluate the outcome $\omega$ of an action
                            (e.g. success or failure) and the <strong>utility</strong> $u$ that such action brings towards achievement of $G$ </li>
                    </ul>
                    <p class="fragment"> The behaviour of rational agents of favouring actions that maximise $u$ is called <strong>preference</strong> </p>
                </ul>

            </section>
            <section>

                <ul>
                    <li class="fragment"> <em>Utility functions</em> are used to model preferences: $u_{i} = \Omega \rightarrow \mathbb{R}$ where $\Omega = {\omega_{1}, ..., \omega_{M}}$ </li>
                    <li class="fragment"> Utility functions enable to define <em>preference orderings</em> over outcomes
                        (hence, actions): $\omega >=_i \omega'$ means $u_i(\omega) >= u_i(\omega')$ as for different
                        agents $u$ and preferences may vary</li>
                    <li class="fragment"> [pic FZ MAS 36] </li>
                </ul>

            </section>
            <section>
                <h3> Multiagent encounters (or, games) </h3>

                <ul>
                    <li class="fragment"> If multiple agents either act (almost) simultaneously or do not have means to
                        observe each other actions, the outcome of their behaviours will be some combination of each
                        individual outcome </li>
                    <li class="fragment"> A <em>state transformer function</em> models such combined outcome on the shared environment: $\tau = Ac_i \times Ac_j \times... \rightarrow \Omega$ </li>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> E.g. considering both agents $i$ and $j$ to only have actions $a_1$ and $a_2$ available ($\tau(Ac_{i} = {a_1, a_2}, Ac_{j} = {a_1, a_2}$): </li>
                    <ol>
                        <li class="fragment"> if $\tau(a_2,a_2) = \omega_1$ and $\tau(a_2,a_1) = \omega_2$ and $\tau(a_1,a_2) = \omega_3$ and $\tau(a_1,a_1) = \omega_4$ then the environment is sensitive to both agents </li>
                        <li class="fragment"> if $\tau(a_2,a_2) = \omega_1$ and $\tau(a_2,a_1) = \omega_1$ and $\tau(a_1,a_2) = \omega_1$ and $\tau(a_1,a_1) = \omega_1$ then the environment is sensitive to none </li>
                        <li class="fragment"> if $\tau(a_2,a_2) = \omega_1$ and $\tau(a_2,a_1) = \omega_2$ and $\tau(a_1,a_2) = \omega_1$ and $\tau(a_1,a_1) = \omega_2$ then the environment is sensitive only to agent $j$ </li>
                    </ol>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> Such state transformer functions let us find preferences, hence define a rational choice </li>
                    <li class="fragment"> Suppose to be in case 1 from previous examples, with utility functions:
                        $$u_i(\omega_1) = 1, u_i(\omega_2) = 1, u_i(\omega_3) = 4, u_i(\omega_4) = 4$$
                        $$u_j(\omega_1) = 1, u_i(\omega_2) = 4, u_i(\omega_3) = 1, u_i(\omega_4) = 4$$ </li>
                    <li class="fragment"> Then, agent $i$ preferences are: $( (a_1,a_1) \geq_i (a_1,a_2) ) \gt_i ( (a_2,a_1) \geq_i (a_2,a_2) )$ </li>
                    <li class="fragment"> Thus, agent $i$ rational choice is $a_1$: every outcome where it does $a_1$ are better than those where he chooses $a_2$, <em>independently</em> of what $j$ does </li>
                </ul>
            </section>
            <section>
                <h3> Dominant strategies </h3>

                <ul>
                    <li class="fragment"> The criteria according to which choosing actions happen reflect the agent <em>strategy</em> (e.g. rational vs random) </li>
                    <li class="fragment"> A strategy $s_1$ is said <em>dominant</em> with respect to $s_2$ if $s_1$ is always preferred to $s_2$ for every possible outcome </li>
                    <li class="fragment"> A rational agent will never play a dominated strategy </li>
                    <li class="fragment"> If there is only one non-dominated strategy, that's the trivial rstrategy to adopt </li>
                </ul>
            </section>
            <section>
                <h3> Zero-sum games </h3>

                <ul>
                    <li class="fragment"> In <em>zero-sum</em> games the utilities add up to $0$: $u_i(\omega) + u_j(\omega) = 0 \forall \omega \in \Omega$ </li>
                    <li class="fragment"> Zero-sum games are strictly competitive: no agent can gain something if no other loses something (every win-lose game basically) </li>
                    <li class="fragment"> In these games there is no rational choice without information about other players' strategies! </li>
                    <li class="fragment"> [pic FZ MAS 44] </li>
                </ul>
            </section>
            <section>
                <h3> Non zero-sum games </h3>

                <ul>
                    <li class="fragment"> Real-life situations are usually non zero-sum: there is always some "compromise" </li>
                    <li class="fragment"> [https://youtu.be/LJS7Igvk6ZM] </li>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> <em>Nash equilibrium</em>: each player's predicted strategy is the
                        best response to the predicted strategies of other players </li>
                    <li class="fragment"> In other words, two strategies $s_1$ and $s_2$ are in Nash equilibrium if: </li>
                    <ul>
                        <li class="fragment"> under the assumption that agent $i$ plays $s_1$, agent $j$ can do no better than play $s_2$ </li>
                        <li class="fragment"> AND under the assumption that agent $j$ plays $s_2$, agent $i$ can do no better than play $s_1$ </li>
                    </ul>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> Unfortunately there are both games with no Nash equilibrium (zero sum ones)
                        and with more than one Nash equilibrium (hence no trivial strategy to adopt)</li>
                    <li class="fragment"> In games where there are common dominant strategies, they represent a Nash equilibrium </li>
                </ul>
            </section>
            <section>
                <h3> Prisoner's dilemma </h3>

                <ul>
                    <li class="fragment"> Two men are collectively charged with a crime and held in separate cells, with no way of
                        meeting or communicating </li>
                    <li class="fragment"> If one confesses (action: defect) and the other does not (action: coop), the confessor will be
                        freed, and the other will be jailed for 3 years </li>
                    <li class="fragment"> If both confess, then each will be jailed for 2 years </li>
                    <li class="fragment"> If none confess, then each will be jailed for 1 year </li>
                    <li class="fragment"> We can measure utilities in term of, e.g., years of prison
                        saved over the case of 4 years of prison </li>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> [pic FZ MAS 50 (payoff matrix)] </li>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> The <em>individual rational action</em> is defect: it guarantees a payoff of
                        no worse than 2, whereas cooperating guarantees a payoff of at most 1 </li>
                    <li class="fragment"> So defect-defect is the Nash Equilibrium (best strategy): both agents defect, and
                        get payoff = 2 </li>
                    <li class="fragment"> But intuition says they should both cooperate and each get payoff of 3! </li>
                </ul>
            </section>
        </section>
        <section data-transition="convex">
            <section data-transition="zoom">

                <h2> Game theory: collective stance </h2>

            </section>
            <section>
                <h3> Game theory paradox </h3>

                <ul>
                    <li class="fragment"> This apparent paradox is the fundamental problem
                        of multi-agent interactions: it <em>appears</em> to imply that cooperation will not occur in
                        societies of self-interested agents </li>
                    <li class="fragment"> The prisoner's dilemma is ubiquitous in real-world problems, too! </li>
                    <li class="fragment"> Can we make agents aware that <em>cooperation</em> is the best strategy? </li>
                </ul>
            </section>
            <section>
                <h3> Axelrod's tournament </h3>

                <ul>
                    <li class="fragment"> A possible answer: play the game <em>repeatedly</em> </li>
                    <li class="fragment"> The incentive to defect evaporates (e.g. due to reputation, fear of retaliation, ...) </li>
                    <li class="fragment"> Cooperation is the rational choice in the <em>indefinitely</em> repeated prisoner's dilemma </li>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> Suppose you play iterated prisoner's dilemma against a set of opponents: what
                        strategy should you choose, so as to maximize your overall (long-term) payoff? </li>
                    <li class="fragment"> Axelrod (1984) investigated this problem, with a computer tournament for programs
                        playing the prisoner's dilemma: many different agents using different strategies, interacting hundreds
                        of times with other agents </li>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> Strategies: </li>
                    <ul>
                        <li class="fragment"> ALLD: "Always defect" (hawk strategy) </li>
                        <li class="fragment"> TIT-FOR-TAT: on round $t_0$ cooperate, then at each $t_{i \gt 0}$ do what opponent
                        did on $t_{i-1}$ </li>
                        <li class="fragment"> TESTER: on 1st round defect, then if opponent retaliated start playing TIT-FOR-TAT,
                            otherwise intersperse cooperation and defection </li>
                        <li class="fragment"> JOSS: as TIT-FOR-TAT, but periodically defect </li>
                    </ul>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> In the long run, TIT-FOR-TAT is best strategy: cooperation wins! </li>
                    <li class="fragment"> Emerging "rules": </li>
                    <ul>
                        <li class="fragment"> Don’t be envious: don'’'t play as if it were zero sum </li>
                        <li class="fragment"> Be nice: start by cooperating, and always reciprocate cooperation immediately </li>
                        <li class="fragment"> Retaliate appropriately: always punish defection immediately, then don't overdue it </li>
                    </ul>
                </ul>
            </section>
            <section>
                <h3> Towards multiagent learning </h3>

                <ul>
                    <li class="fragment"> Axelrod Tournament shows that a group of agents can change behaviour (i.e.,
                        strategy), to eventually <em>learn</em> what is the most suitable way of behaving to maximize own
                        success or/while maximize overall success of the group </li>
                </ul>
            </section>
        </section>
        <section data-transition="convex">
            <!--<section data-transition="zoom">

                <h3> Multiagent systems </h3>

                <ul>
                    <li class="fragment"> Autonomous individuals vs. autonomous systems (FZ: slide 20 CAS) </li>
                    <li class="fragment"> cfr. swarm intelligence (FZ: slide 4 swarm) </li>
                    <li class="fragment"> AO: slide 52,53 C8 </li>
                    <li class="fragment"> FZ: NetLogo overview and example (?) </li>
                </ul>

            </section>-->
            <section data-transition="zoom">

                <h2> Multiagent interactions </h2>

            </section>
            <section>
                <h3> Forms of interaction </h3>

                <ul>
                    <li class="fragment"> Differently from game theory, let's now assume that agents can interact: </li>
                    <ul>
                        <li class="fragment"> <em>Direct</em> interactions: agents directly communicate with each other by exchanging messages </li>
                        <li class="fragment"> <em>Indirect</em> (or stigmergic) interactions: agents interact indirectly by accessing
                            an environment in which they situate (physical or computational or mixed) </li>
                        <li class="fragment"> As they have a mean to affect each other's actions and beliefs, they can strategically act
                            based on what the other agents do (observation), and agree on common courses of actions </li>
                    </ul>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> Interactions may imply: </li>
                    <ul>
                        <li class="fragment"> <em>Communication</em>: explicit exchange of beliefs, desires, intentions, ... </li>
                        <li class="fragment"> <em>Synchronization</em>: ordering of concurrent actions </li>
                        <li class="fragment"> <em>Coordination</em>: organizing a shared plan of actions </li>
                    </ul>
                </ul>
            </section>
            <section>
                <h3> Types of interaction </h3>

                <ul>
                    <li class="fragment"> <em>Collaborative</em>: agents cooperate towards the achievement of some common
                        application goals, and trust each other because they know they work towards the same goal </li>
                    <li class="fragment"> <em>Competitive</em>: agents are forced to interact to achieve individual goals, but
                        they don't have to trust each other </li>
                    <li class="fragment"> <em>Collaborative competition</em>: agents compete but for the sake of achieving
                        some global goal, hence they trust each other </li>
                </ul>
            </section>
            <section>
                <h3> MAS vs. distributed algorithms </h3>

                <ul>
                    <li class="fragment"> In distributed algorithms, too, there is need of reaching agreement on how to
                        act or on a common perspective of the world (leader election, mutual exclusion, validation of a
                        blockchain transaction, etc.) </li>
                    <li class="fragment"> However, there is no concept such as "goal", "utility" of actions, etc.: either
                        the algorithms works, or fails </li>
                </ul>
            </section>
            <section>
                <h3> Interaction protocol </h3>

                <ul>
                    <li class="fragment"> Defines the <em>rules</em> of the encounter between agents: the set of available interaction
                        actions, their dependencies, how they affect the "state of the world", etc. </li>
                    <li class="fragment"> Direct communication: sequence of messages between agents aimed at discussing
                        how to reach agreement </li>
                    <li class="fragment"> Stigmergic interactions: the form and structure of the "signs" in the environment,
                        how and when agents should leave signs, and how they should react to these </li>
                    <li class="fragment"> In this context, agents should abide to the "rules of the game", and their
                        strategy regards freedom in deciding what to do at each step of the protocol (amongst admissible actions) </li>
                </ul>
            </section>
            <section>
                <ul>
                    <li class="fragment"> E.g. "battle of the sexes": </li>
                    <ol>
                        <li class="fragment"> MAN: "Football!" </li>
                        <li class="fragment"> WOMAN: "Movie!" </li>
                        <li class="fragment"> MAN: "This time Football next time Movie" </li>
                        <li class="fragment"> WOMAN: "No, this time movie and next time Football" </li>
                        <li class="fragment"> MAN: "No, This time Football next TWO times Movie" </li>
                        <li class="fragment"> WOMAN: "OK" </li>
                        <li class="fragment"> MAN: "OK" </li>
                    </ol>
                </ul>
            </section>
            <section>
                <h3> Protocol (interaction space) design </h3>

                <ul>
                    <li class="fragment"> Designing an interaction protocol implies devising a sequence of interaction actions
                        that has desired properties </li>
                    <li class="fragment"> E.g. battle of the sexes protocol: </li>
                    <ol>
                        <li class="fragment"> MAN: Proposal </li>
                        <li class="fragment"> WOMAN: Proposal </li>
                        <li class="fragment"> If action differs: </li>
                        <ol>
                            <li class="fragment"> MAN: agree or counter-proposal </li>
                            <li class="fragment"> WOMAN: agree or counter-proposal </li>
                        </ol>
                        <li class="fragment"> While (agree < 2) </li>
                        <ol>
                            <li class="fragment"> MAN: agree or counter-proposal with increased WOMAN utility </li>
                            <li class="fragment"> WOMAN: agree or counter-proposal with increased MAN utility </li>
                        </ol>
                    </ol>
                    <li class="fragment"> Strategy: as soon as the counter-proposal reaches a sufficiently high utility,
                        agree </li>
                    <li class="fragment"> <em>Collaborative or competitive?</em> </li>
                </ul>
            </section>
            <section>
                <h3> Direct interaction: negotiation </h3>

                <ul>
                    <li class="fragment"> "Negotiation is an economically-inspired form of <em>distributed decision making</em> where two or more
                        partners jointly search a space of possible solutions to reach a common consensus" (P. Maes) </li>
                    <ul>
                        <li class="fragment"> Collaborative --> ContractNet, argumentation </li>
                        <li class="fragment"> Collaborative --> Auctions </li>
                    </ul>
                    <li class> Applications </li>
                    <uL>
                        <li class="fragment"> Cellular networks </li>
                        <li class="fragment"> Manufacturing </li>
                        <li class="fragment"> Supply chain </li>
                    </uL>
                </ul>
            </section>
            <section>
                <h3> ContractNet </h3>

                <ul>
                    <li class="fragment"> An agent (initiator) needs help from others to carry out a set of tasks (because it cannot
                        or prefer not to do them itself), hence informs other agents about these tasks (type,
                        description, requirements, deadline, etc) and asks them to PROPOSE to perform them </li>
                    <li class="fragment"> Other agents (responders) place "bids" specifying how they would be able to carry out
                        such task (in what time, with what efficiency and accuracy, etc) </li>
                    <li class="fragment"> The initiator then assigns each task to the "best" responder (according to
                        whatever criteria / strategy) </li>
                </ul>
            </section>
            <section>
                <h3> Auctions </h3>

                <ul>
                    <li class="fragment"> When agents have competing interests and no interest
                        in cooperating, the only solution for cooperation is to "pay" the actions/tasks/resources
                        that agents provide to others </li>
                    <li class="fragment"> Auctions are the negotiation mechanisms that determines
                        the <em>values</em> of a good/resource/actions to be "sold" by an
                        offering (seller) agents to buyer agent(s) </li>
                </ul>
            </section>
            <section>
                <h3> Indirect (mediated) interaction </h3>

                <ul>
                    <li class="fragment"> All the form of direct interaction seen can be replicated as indirect ones,
                    based on some sort of environment mediation </li>
                    <li class="fragment"> Tuple spaces: shared repositories of data with access mechanisms embedding
                    synchronisation of interactions </li>
                    <li class="fragment"> Pheromone-based coordination: inspired by collective behaviour of social insects
                    such as ants </li>
                    <li class="fragment"> Field-based coordination: inspired by gravitational waves influencing how
                    objects behave </li>
                </ul>
            </section>
            <section>
                <h3> The tuple space model </h3>

                [pic]

            </section>
        </section>
        <section>
            <h3> JADE ContractNet (?) </h3>


        </section>
        <section>
            <h3> TuCSoN / Tusow (?) </h3>


        </section>
        <section data-transition="fade">
            <section data-autoslide="2000">

                <br/>
                <h2> Thanks </h2>
                <h2> for your attention </h2>
                <br/>
                <br/>
                <h4 class="fragment fade-in-then-semi-out"> Questions? </h4>
                <p class="fragment"></p>
                <br/>
                <p><a href="http://stefanomariani.apice.unibo.it"> Stefano Mariani </a></p>
                <p><small><em> Università di Modena e Reggio Emilia </em></small></p>

            </section>
            <!--<section data-autoslide="3000">

                <h3> References </h3>

            </section>-->
        </section>
    </div>
</div>

<script src="js/reveal.js"></script>
<script src="plugin/math/math.js"></script>

<script>
    Reveal.initialize({
        autoSlide: 1000,
        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: 120,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        pdfSeparateFragments: false, // http://localhost:63342/reveal.js/?print-pdf/&showNotes=true
        dependencies: [
            {src: 'plugin/markdown/marked.js'},
            {src: 'plugin/markdown/markdown.js'},
            {src: 'plugin/zoom-js/zoom.js'},
            {src: 'plugin/notes/notes.js'},
            //{src: 'plugin/highlight/highlight.js', async: true}
        ],
        plugins: [ RevealMath ],
        slideNumber: true
    });
</script>

</body>
</html>
